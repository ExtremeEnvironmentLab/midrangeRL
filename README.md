
## 项目介绍

你好啊，赛博飞行员，你是否时常对这些问题感到疑惑：中距弹的射程为什么一会儿写10千米一会儿写100千米？现代导弹都能拉出30G过载了，战斗机的机动性还有意义吗？

如果你希望这些困惑得到解答，那么欢迎来到midrangeRL。它是一个娱乐+科普性质的中距空战AI强化学习平台，最大的特色是“时空图”UI，用于估计导弹真实射程。

在这里，你可以设计和测试你的空战策略，同时还还可以手动游玩，亲自体验与各式各样的空战AI对抗。物理参数的真实性调试也是一项很好玩的工作，后面我们还会引入更多的物理机制，比如更科学的气动计算、超音速气动修正、雷达RCS等。这些都需要你根据在安东星激情对射的经验来提出批评与改进建议！

Gemini、Cursor与TRAE等LLM代码助手正在帮我们处理最令人掉头发的程序实现部分。

## 主要文件介绍

### config默认参数文件：
1. 飞机和导弹的物理参数，最小转弯半径R，自由落体终端速度vt，诱导阻力Cd1函数规则（默认是与舵量成正比），导弹发动机工作时间Rocket_lifetime
2. 更多导弹制导策略（计划中的开发）
3. 默认训练策略（计划中的开发）

### requirements.txt需提前安装好的库：
Pytorch、RLlib、gym

### env_tensor游戏环境后端文件夹：
1. 基于Pytorch内置tensor方法实现，可以被train与play调用而在GPU上创建若干个游戏实例
2. 训练模式自动高倍率时间加速，让GPU主频与作战Ai的模型性能来决定一秒钟跑多少个物理帧
3. aerodynamic_tensor.py ：这个文件原本叫做aircraft_model，它的功能是从config获得导弹和飞机的物理参数，然后进行角速度和加速度两项物理计算，角速度Omega正比于线速度v乘以舵量rudder除以最小转弯半径R
4. 加速度a=推力Thrust-阻力Drag，Drag=阻力系数Cd*速度v²，Cd=零升阻力系数Cd0+诱导阻力系数Cd1，Cd0=重力加速度g/自由落体终端速度vt平方
5. missile_guidance_tensor.py ：它定义了导弹制导模式是比例引导法
6. game_events_tensor.py 这个文件原本是主程序midrangeRL的一部分，用于处理导弹发射、导弹速度低于200自毁、导弹摧毁战机、游戏胜利事件、同归于尽事件、武器耗尽事件

### env_numpy游戏环境后端文件夹：
1. 包含aerodynamic.py、game_events.py、missile_guidance.py，这是最初版本的游戏后端，当使用tensor运算的GPU版env多次启动失败时，建议您通过修改game_play.py和visualization.py文件当中的调用，手动切换到这个使用CPU计算的后端

### game_play.py游戏主程序：（原midrangeRL.py）
1. 开始菜单当中可设置红蓝双方各自为Ai还是玩家，可以选择Ai模型的版本
2. 通过visualization.py提供可视化UI，画风模仿了3Blue1Brown的科普视频制作库Manim，可切换到时空图视角，展示单个env实例中发生的事，并显示胜利/同归于尽/平局
3. 向env传递玩家操作（红方AD舵量，T开火，蓝方左右舵量，=开火）
4. 也可以传递Ai模型的操作，同样是舵量与开火


### replays文件夹：（计划中的开发）
1. 存放战斗录像（为了避免这个文件夹爆满，game_play默认不开启录像，需要每次打开游戏时手动开启）
2. game_play当中可以回到开始菜单查看已保存的战斗回放
3. 当游戏进行时/回放播放时game_play的“实体列表”UI中以示波器形式展示各个实体的速度大小随时间变化的曲线

### models文件夹：（计划中的开发）
1. 存放不同结构的Ai模型设计与模型参数，比如全连接、时空图扩散预测器、离散动作组等
2. 模型参数文件的命名规则为“模型名称_版本号”，比如Dense16neuro3layer_v1
3. 有一个完全基于预设规则的AI，智商比较低，懂得追击和在射程内发射导弹，但是一看到敌方导弹来袭就特别喜欢逃跑，逃跑的时候会乱扭消耗自己的速度，可以用在游戏中调戏，也可以给别的模型冷启动

### train.py强化学习训练程序：（计划中的开发）
1. 提供训练选项菜单，可以设置训练哪一个模型，使用哪一个版本的奖励函数，是否选择已训练过的版本，训练步数要多少步，训练完成后保存的文件名称等
2. 可根据GPU性能预估可以并行的实例数，无论是弱小可怜无助的1050还是豪华的V100都可以稳定发挥出50%以上的性能，听风扇呼啸
3. 提供训练过程中参数变化的UI，以曲线图形式显示，并显示预期还剩多少分钟完成训练

### evaluate.py评估程序：
（计划中的开发）

### distill.py蒸馏程序：
（计划中的开发，用于预处理那些在强化学习刚开始时难以冷启动收敛的模型，方法是用已经收敛的模型生成replay数据集，构造损失函数做梯度下降来训练新模型）

### RL_tools强化学习工具箱文件夹：
1. （计划中的开发）rewards_v1.py 这是默认版本的奖励函数，包括自保、击中自己的导弹的速度、杀敌、节约导弹，这里我们十分鼓励你自己设计一个版本


## 路线图
✅ 完成基础 NumPy 环境 (env_numpy) 并验证核心物理逻辑 (基于力)。

🚧 实现 PyTorch Tensor 后端 (env_tensor) 以支持 GPU 并行。

🚀 对接强化学习框架，实现基础训练流程 (train.py)。

✨ 开发 HTML5 前端原型，特别是时空图可视化 (web_frontend)。

🔧 (可选/评估中) 探索 Isaac Gym 以获得极致并行性能。

📦 模型导出与集成，完成可玩的 HTML5 版本。

📈 持续迭代物理模型、AI 策略和功能。

## 许可证
MIT License
